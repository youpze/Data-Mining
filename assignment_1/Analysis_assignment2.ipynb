{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca254db0-81c6-46c4-a66e-9b7f66172755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f7bcaa-d6d3-49c1-b9b0-5edec2d3a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files changes path to where the data is stored\n",
    "test = pd.read_csv(r\"C:\\Users\\youpz\\Documents\\Master\\P5\\Data mining techniques\\Assignment2\\data\\dmt-2025-2nd-assignment\\test_set_VU_DM.csv\")\n",
    "train = pd.read_csv(r\"C:\\Users\\youpz\\Documents\\Master\\P5\\Data mining techniques\\Assignment2\\data\\dmt-2025-2nd-assignment\\training_set_VU_DM.csv\")\n",
    "sample = pd.read_csv(r\"C:\\Users\\youpz\\Documents\\Master\\P5\\Data mining techniques\\Assignment2\\data\\dmt-2025-2nd-assignment\\submission_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e6cd88-2e01-44c2-92b9-3216c163684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199549 number of search IDs in sample\n",
      "129438 number of different hotels in sample\n",
      "199549 number of search ids in testdataset\n"
     ]
    }
   ],
   "source": [
    "#number op srch_id\n",
    "print(len(sample['srch_id'].unique()), 'number of search IDs in sample')\n",
    "print(len(sample['prop_id'].unique()), 'number of different hotels in sample')\n",
    "print(len(test['srch_id'].unique()),'number of search ids in testdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbcc70d-5314-4a96-aede-85eedee0e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srch_id sets match between sample and test.\n",
      "Mismatch in srch_ids between sample and train! Missing in sample: 79881, Missing in train: 79635\n"
     ]
    }
   ],
   "source": [
    "# Check if srch_ids match between sample, test, and train --> to get a feeling for the code\n",
    "\n",
    "sample_ids = set(sample['srch_id'].unique())\n",
    "test_ids = set(test['srch_id'].unique())\n",
    "train_ids = set(train['srch_id'].unique())\n",
    "\n",
    "# Check sample vs. test\n",
    "if sample_ids != test_ids:\n",
    "    missing_in_sample = test_ids - sample_ids\n",
    "    missing_in_test = sample_ids - test_ids\n",
    "    print(f\"Mismatch in srch_ids between sample and test! Missing in sample: {len(missing_in_sample)}, Missing in test: {len(missing_in_test)}\")\n",
    "else:\n",
    "    print(\"srch_id sets match between sample and test.\")\n",
    "\n",
    "# Check sample vs. train\n",
    "if sample_ids != train_ids:\n",
    "    missing_in_sample_train = train_ids - sample_ids\n",
    "    missing_in_train_sample = sample_ids - train_ids\n",
    "    print(f\"Mismatch in srch_ids between sample and train! Missing in sample: {len(missing_in_sample_train)}, Missing in train: {len(missing_in_train_sample)}\")\n",
    "else:\n",
    "    print(\"srch_id sets match between sample and train.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a5d234-6885-41df-bdf7-94794f3df4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "    \"\"\"Returns a DataFrame with missing counts and percent missing for each column.\"\"\"\n",
    "    n = len(df)\n",
    "    missing_count   = df.isna().sum()\n",
    "    missing_percent = 100 * missing_count / n\n",
    "    missing_df = (\n",
    "        pd.DataFrame({\n",
    "            'missing_count':   missing_count,\n",
    "            'missing_percent': missing_percent\n",
    "        })\n",
    "        .sort_values('missing_percent', ascending=False)\n",
    "    )\n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0e5c47-2faf-4d18-9955-94abd8b44a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           missing_count  missing_percent\n",
      "comp1_rate_percent_diff          4863908        98.095353\n",
      "comp6_rate_percent_diff          4862173        98.060362\n",
      "comp1_rate                       4838417        97.581250\n",
      "comp1_inv                        4828788        97.387053\n",
      "comp4_rate_percent_diff          4827261        97.356256\n",
      "gross_bookings_usd               4819957        97.208949\n",
      "comp7_rate_percent_diff          4819832        97.206428\n",
      "comp6_rate                       4718190        95.156511\n",
      "visitor_hist_starrating          4706481        94.920364\n",
      "visitor_hist_adr_usd             4705359        94.897735\n",
      "comp6_inv                        4697371        94.736633\n",
      "comp4_rate                       4650969        93.800797\n",
      "comp7_rate                       4642999        93.640058\n",
      "srch_query_affinity_score        4640941        93.598552\n",
      "comp4_inv                        4614684        93.069001\n",
      "comp7_inv                        4601925        92.811677\n",
      "comp3_rate_percent_diff          4485550        90.464625\n",
      "comp2_rate_percent_diff          4402109        88.781786\n",
      "comp8_rate_percent_diff          4343617        87.602118\n",
      "comp5_rate_percent_diff          4117248        83.036706\n",
      "                           missing_count  missing_percent\n",
      "comp1_rate_percent_diff          4863908        98.095353\n",
      "comp6_rate_percent_diff          4862173        98.060362\n",
      "comp1_rate                       4838417        97.581250\n",
      "comp1_inv                        4828788        97.387053\n",
      "comp4_rate_percent_diff          4827261        97.356256\n",
      "gross_bookings_usd               4819957        97.208949\n",
      "comp7_rate_percent_diff          4819832        97.206428\n",
      "comp6_rate                       4718190        95.156511\n",
      "visitor_hist_starrating          4706481        94.920364\n",
      "visitor_hist_adr_usd             4705359        94.897735\n",
      "comp6_inv                        4697371        94.736633\n",
      "comp4_rate                       4650969        93.800797\n",
      "comp7_rate                       4642999        93.640058\n",
      "srch_query_affinity_score        4640941        93.598552\n",
      "comp4_inv                        4614684        93.069001\n",
      "comp7_inv                        4601925        92.811677\n",
      "comp3_rate_percent_diff          4485550        90.464625\n",
      "comp2_rate_percent_diff          4402109        88.781786\n",
      "comp8_rate_percent_diff          4343617        87.602118\n",
      "comp5_rate_percent_diff          4117248        83.036706\n",
      "comp3_rate                       3424059        69.056462\n",
      "comp3_inv                        3307357        66.702814\n",
      "comp8_rate                       3041693        61.344900\n",
      "comp8_inv                        2970844        59.916016\n",
      "comp2_rate                       2933675        59.166392\n",
      "comp2_inv                        2828078        57.036710\n",
      "comp5_rate                       2735974        55.179155\n",
      "comp5_inv                        2598327        52.403089\n",
      "orig_destination_distance        1607782        32.425766\n"
     ]
    }
   ],
   "source": [
    "mv = missing_values_table(train)\n",
    "print(mv.head(20))       # top 20 most‐missing columns\n",
    "# Or to filter down to “lots” of missing, say >30%:\n",
    "print(mv[mv['missing_percent'] > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea75187-7171-44a8-b8d1-ef210c3b2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## some beginning on the feature engineering\n",
    "\n",
    "\n",
    "def preprocess_missing_and_competitors(train_df, test_df):\n",
    "    # 1) Drop features with >93% missing or that leak the target\n",
    "    drop_cols = [\n",
    "        # competitor 1,4,6,7 are ~97–98% missing → too sparse to learn\n",
    "        *[f'comp{i}_{t}' for i in [1,4,6,7] for t in ['rate','inv','rate_percent_diff']],\n",
    "        'gross_bookings_usd'  # only in train, leaks booking price\n",
    "    ]\n",
    "    train_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "    test_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "\n",
    "    # 2) Impute & flag user history features\n",
    "    #    Missing means “no prior purchases” → keep with sentinel + flag\n",
    "    for df in (train_df, test_df):\n",
    "        # visitor_hist_starrating\n",
    "        df['hist_star_na'] = df['visitor_hist_starrating'].isna().astype(int)\n",
    "        # fill with median starrating across users\n",
    "        star_med = train_df['visitor_hist_starrating'].median()\n",
    "        df['visitor_hist_starrating'] = df['visitor_hist_starrating'].fillna(star_med)\n",
    "\n",
    "        # visitor_hist_adr_usd (avg USD spend)\n",
    "        df['hist_adr_na'] = df['visitor_hist_adr_usd'].isna().astype(int)\n",
    "        adr_med = train_df['visitor_hist_adr_usd'].median()\n",
    "        df['visitor_hist_adr_usd'] = df['visitor_hist_adr_usd'].fillna(adr_med)\n",
    "\n",
    "    # 3) Impute & flag affinity score\n",
    "    #    Null means “hotel never seen” → fill with global minimum and flag\n",
    "    affinity_min = train_df['srch_query_affinity_score'].min(skipna=True)\n",
    "    for df in (train_df, test_df):\n",
    "        df['affinity_na'] = df['srch_query_affinity_score'].isna().astype(int)\n",
    "        df['srch_query_affinity_score'] = (\n",
    "            df['srch_query_affinity_score']\n",
    "            .fillna(affinity_min)\n",
    "        )\n",
    "\n",
    "    # 4) Keep & impute competitor 2,3,5,8 features (~50–90% missing)\n",
    "    #    Null → “no data” sentinel (for categorical) or 0 (for percent diff), plus flag\n",
    "    keep_comps = [2,3,5,8]\n",
    "    for i in keep_comps:\n",
    "        # availability flag\n",
    "        inv_col = f'comp{i}_inv'\n",
    "        flag_col = f'comp{i}_inv_na'\n",
    "        for df in (train_df, test_df):\n",
    "            df[flag_col] = df[inv_col].isna().astype(int)\n",
    "            # fill null with 2 (new category: 0=no avail,1=avail,2=no data)\n",
    "            df[inv_col] = df[inv_col].fillna(2).astype(int)\n",
    "\n",
    "        # price‐compare flag\n",
    "        rate_col = f'comp{i}_rate'\n",
    "        rate_flag = f'comp{i}_rate_na'\n",
    "        for df in (train_df, test_df):\n",
    "            df[rate_flag] = df[rate_col].isna().astype(int)\n",
    "            # fill null as “no data” = 2\n",
    "            df[rate_col] = df[rate_col].fillna(2).astype(int)\n",
    "\n",
    "        # percent_diff\n",
    "        pdiff_col = f'comp{i}_rate_percent_diff'\n",
    "        pdiff_flag = f'comp{i}_pdiff_na'\n",
    "        for df in (train_df, test_df):\n",
    "            df[pdiff_flag] = df[pdiff_col].isna().astype(int)\n",
    "            # fill null as 0% diff (no info)\n",
    "            df[pdiff_col] = df[pdiff_col].fillna(0.0)\n",
    "\n",
    "    # 5) Bucket orig_destination_distance\n",
    "    #    Missing → sentinel bucket + flag\n",
    "    for df in (train_df, test_df):\n",
    "        df['dist_na'] = df['orig_destination_distance'].isna().astype(int)\n",
    "        df['orig_destination_distance'] = (\n",
    "            df['orig_destination_distance'].fillna(-1)\n",
    "        )\n",
    "        # define bins (in km)\n",
    "        bins = [-1, 0, 10, 50, 200, np.inf]\n",
    "        labels = ['missing','0-10km','10-50km','50-200km','200km+']\n",
    "        df['dist_bucket'] = pd.cut(\n",
    "            df['orig_destination_distance'],\n",
    "            bins=bins, labels=labels\n",
    "        )\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def create_base_features(df):\n",
    "    \"\"\"1) Parse datetime & basic price/historical features.\"\"\"\n",
    "    df = df.copy()\n",
    "    # --- Date/time splits ---\n",
    "    df['date_time']   = pd.to_datetime(df['date_time'])\n",
    "    df['search_year'] = df['date_time'].dt.year\n",
    "    df['search_month']= df['date_time'].dt.month\n",
    "    df['search_day']  = df['date_time'].dt.day\n",
    "    df['search_hour'] = df['date_time'].dt.hour\n",
    "\n",
    "    # --- Price per night & hist price devation ---\n",
    "    df['price_per_night'] = df['price_usd'] / df['srch_length_of_stay']\n",
    "    # fill missing visitor_hist_adr with median later in preprocessing\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_destination_stats(train_df, test_df):\n",
    "    \"\"\"6) Dest‑level total searches & booking rate.\"\"\"\n",
    "    dest = (\n",
    "        train_df\n",
    "        .groupby('srch_destination_id')\n",
    "        .agg(dest_searches=('srch_id','count'),\n",
    "             dest_bookings=('booking_bool','sum'))\n",
    "        .assign(dest_booking_rate=lambda x: x.dest_bookings / x.dest_searches)\n",
    "        .reset_index()\n",
    "    )\n",
    "    # Reassign merge result back to each DataFrame\n",
    "    train_df = train_df.merge(\n",
    "        dest[['srch_destination_id','dest_searches','dest_booking_rate']],\n",
    "        on='srch_destination_id', how='left'\n",
    "    )\n",
    "    test_df = test_df.merge(\n",
    "        dest[['srch_destination_id','dest_searches','dest_booking_rate']],\n",
    "        on='srch_destination_id', how='left'\n",
    "    )\n",
    "    return train_df, test_df\n",
    "\n",
    "def add_within_search_features(df):\n",
    "    \"\"\"7) Z‑scores & deltas in each search group.\"\"\"\n",
    "    grp = df.groupby('srch_id')\n",
    "    # price\n",
    "    df['price_mean_srch'] = grp['price_usd'].transform('mean')\n",
    "    df['price_std_srch']  = grp['price_usd'].transform('std').fillna(1)\n",
    "    df['price_zscore']    = (df['price_usd'] - df['price_mean_srch']) / df['price_std_srch']\n",
    "    # stars\n",
    "    df['star_mean_srch']  = grp['prop_starrating'].transform('mean')\n",
    "    df['star_delta_srch'] = df['prop_starrating'] - df['star_mean_srch']\n",
    "    # user delta\n",
    "    df['star_delta_user'] = df['prop_starrating'] - df['visitor_hist_starrating']\n",
    "    # distance\n",
    "    df['dist_mean_srch']  = grp['orig_destination_distance'].transform('mean')\n",
    "    df['dist_std_srch']   = grp['orig_destination_distance'].transform('std').fillna(1)\n",
    "    df['dist_zscore']     = (df['orig_destination_distance'] - df['dist_mean_srch']) / df['dist_std_srch']\n",
    "    return df\n",
    "\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"8) Weekday/weekend & check‑in weekend flags.\"\"\"\n",
    "    # day‑of‑week for search\n",
    "    df['search_dow'] = df['date_time'].dt.weekday  # 0=Mon…6=Sun\n",
    "    df['is_search_weekend'] = df['search_dow'].isin([5,6]).astype(int)\n",
    "    # approximate check‑in day\n",
    "    checkin = df['date_time'] + pd.to_timedelta(df['srch_booking_window'], 'D')\n",
    "    df['checkin_dow'] = checkin.dt.weekday\n",
    "    df['is_checkin_weekend'] = df['checkin_dow'].isin([5,6]).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_ranks(df):\n",
    "    \"\"\"9) Dense ranks of price, star & distance within each search.\"\"\"\n",
    "    df['price_rank'] = df.groupby('srch_id')['price_usd'].rank('dense', ascending=True)\n",
    "    df['star_rank']  = df.groupby('srch_id')['prop_starrating'].rank('dense', ascending=False)\n",
    "    df['dist_rank']  = df.groupby('srch_id')['orig_destination_distance'].rank('dense', ascending=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7450a3-d266-4bd7-8050-8105ed6ee139",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, test_feat = preprocess_missing_and_competitors(train, test)\n",
    "\n",
    "# create_base_features returns a new DataFrame, so we need to capture it\n",
    "train_feat = create_base_features(train_feat)\n",
    "test_feat  = create_base_features(test_feat)\n",
    "\n",
    "train_feat, test_feat = add_destination_stats(train_feat, test_feat)\n",
    "\n",
    "train_feat = add_within_search_features(train_feat)\n",
    "train_feat = add_temporal_features(train_feat)\n",
    "train_feat = add_ranks(train_feat)\n",
    "\n",
    "test_feat = add_within_search_features(test_feat)\n",
    "test_feat = add_temporal_features(test_feat)\n",
    "test_feat = add_ranks(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50068faf-f786-4887-86e5-85d3fc98dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### create train and test data\n",
    "\n",
    "# Create relevance label for training\n",
    "y = train_feat['booking_bool'] * 5 + train_feat['click_bool']\n",
    "\n",
    "# List of features to drop (train-only or unneeded)\n",
    "drop_cols = ['date_time','click_bool','booking_bool','gross_bookings_usd','position']\n",
    "features = [c for c in train_feat.columns if c not in drop_cols + ['srch_id','prop_id']]\n",
    "\n",
    "#create X train and X_test\n",
    "X = train_feat[features]\n",
    "X_test = test_feat[features]\n",
    "\n",
    "\n",
    "# Extract unique query IDs so we can split at the search-session level\n",
    "group_ids = train_feat['srch_id'].unique()\n",
    "\n",
    "# train test split 80/20\n",
    "train_ids, valid_ids = train_test_split(\n",
    "    group_ids,\n",
    "    test_size=0.2,\n",
    "    random_state=22\n",
    ")\n",
    "\n",
    "# Build boolean masks that mark every row in train_feat as belonging\n",
    "#    either to the train split or the validation split, based on its srch_id\n",
    "mask_tr = train_feat['srch_id'].isin(train_ids)\n",
    "mask_va = train_feat['srch_id'].isin(valid_ids)\n",
    "\n",
    "# Subset your feature matrix X and label vector y according to those masks\n",
    "#    so X_tr/y_tr and X_va/y_va correspond to disjoint sets of queries\n",
    "X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "X_va, y_va = X[mask_va], y[mask_va]\n",
    "\n",
    "# Compute the “group sizes” for LightGBM’s ranker: for each query (srch_id),\n",
    "#    count how many candidate rows belong to it.  This array of counts\n",
    "#    tells the ranker where one query ends and the next begins.\n",
    "groups_tr = train_feat[mask_tr].groupby('srch_id').size().values\n",
    "groups_va = train_feat[mask_va].groupby('srch_id').size().values\n",
    "\n",
    "#get train and valdiation data to train the model\n",
    "train_data = lgb.Dataset(X_tr, label=y_tr, group=groups_tr)\n",
    "valid_data = lgb.Dataset(X_va, label=y_va, group=groups_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28bac05-c5ab-4228-9560-57df25d77448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's ndcg@5: 0.378551\n",
      "[200]\tvalid_0's ndcg@5: 0.385603\n",
      "[300]\tvalid_0's ndcg@5: 0.388075\n",
      "[400]\tvalid_0's ndcg@5: 0.389296\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's ndcg@5: 0.389508\n"
     ]
    }
   ],
   "source": [
    "#run lgb model and set paramters\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "#dont make the model run endlesly \n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[valid_data],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)],\n",
    "    num_boost_round=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13aaf33-0939-4833-97ed-6f0ef880c3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3abc2f5-522b-4ec1-815f-e0d83cd1c2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission.csv adjusted with new scores!\n"
     ]
    }
   ],
   "source": [
    "# Predict relevance scores for each test row\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Insert those scores into the sample submission DataFrame\n",
    "sample['score'] = preds\n",
    "\n",
    "#  Sort by search session (ascending) and score (descending)\n",
    "#    so that for each srch_id, the most relevant prop_id comes first\n",
    "submission = sample.sort_values(\n",
    "    ['srch_id', 'score'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# keep only the required columns and write to CSV\n",
    "#    Kaggle expects: srch_id, prop_id (in ranked order)\n",
    "submission[['srch_id', 'prop_id']].to_csv(\n",
    "    'submission.csv',\n",
    "    index=False\n",
    ")\n",
    "print(\"Submission.csv adjusted with new scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c657572-49a0-41a5-8202-9ed7560edb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
