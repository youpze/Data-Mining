{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca254db0-81c6-46c4-a66e-9b7f66172755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f7bcaa-d6d3-49c1-b9b0-5edec2d3a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files changes path to where the data is stored\n",
    "test = pd.read_csv(r\"C:\\Users\\youpz\\Documents\\Master\\P5\\Data mining techniques\\Assignment2\\data\\dmt-2025-2nd-assignment\\test_set_VU_DM.csv\")\n",
    "train = pd.read_csv(r\"C:\\Users\\youpz\\Documents\\Master\\P5\\Data mining techniques\\Assignment2\\data\\dmt-2025-2nd-assignment\\training_set_VU_DM.csv\")\n",
    "sample = pd.read_csv(r\"C:\\Users\\youpz\\Documents\\Master\\P5\\Data mining techniques\\Assignment2\\data\\dmt-2025-2nd-assignment\\submission_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e6cd88-2e01-44c2-92b9-3216c163684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199549 number of search IDs in sample\n",
      "129438 number of different hotels in sample\n",
      "199549 number of search ids in testdataset\n"
     ]
    }
   ],
   "source": [
    "#number op srch_id\n",
    "print(len(sample['srch_id'].unique()), 'number of search IDs in sample')\n",
    "print(len(sample['prop_id'].unique()), 'number of different hotels in sample')\n",
    "print(len(test['srch_id'].unique()),'number of search ids in testdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbcc70d-5314-4a96-aede-85eedee0e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srch_id sets match between sample and test.\n",
      "Mismatch in srch_ids between sample and train! Missing in sample: 79881, Missing in train: 79635\n"
     ]
    }
   ],
   "source": [
    "# Check if srch_ids match between sample, test, and train --> to get a feeling for the code\n",
    "\n",
    "sample_ids = set(sample['srch_id'].unique())\n",
    "test_ids = set(test['srch_id'].unique())\n",
    "train_ids = set(train['srch_id'].unique())\n",
    "\n",
    "# Check sample vs. test\n",
    "if sample_ids != test_ids:\n",
    "    missing_in_sample = test_ids - sample_ids\n",
    "    missing_in_test = sample_ids - test_ids\n",
    "    print(f\"Mismatch in srch_ids between sample and test! Missing in sample: {len(missing_in_sample)}, Missing in test: {len(missing_in_test)}\")\n",
    "else:\n",
    "    print(\"srch_id sets match between sample and test.\")\n",
    "\n",
    "# Check sample vs. train\n",
    "if sample_ids != train_ids:\n",
    "    missing_in_sample_train = train_ids - sample_ids\n",
    "    missing_in_train_sample = sample_ids - train_ids\n",
    "    print(f\"Mismatch in srch_ids between sample and train! Missing in sample: {len(missing_in_sample_train)}, Missing in train: {len(missing_in_train_sample)}\")\n",
    "else:\n",
    "    print(\"srch_id sets match between sample and train.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea75187-7171-44a8-b8d1-ef210c3b2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## some beginning on the feature engineering\n",
    "\n",
    "def create_base_features(df):\n",
    "    # Parse date_time into components\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df['search_year']  = df['date_time'].dt.year\n",
    "    df['search_month'] = df['date_time'].dt.month\n",
    "    df['search_day']   = df['date_time'].dt.day\n",
    "    df['search_hour']  = df['date_time'].dt.hour\n",
    "\n",
    "    # Price per night\n",
    "    df['price_per_night'] = df['price_usd'] / df['srch_length_of_stay']\n",
    "\n",
    "    # Historical price deviation\n",
    "    hist_mean = df['visitor_hist_adr_usd'].mean()\n",
    "    df['hist_price_usd'] = df['visitor_hist_adr_usd'].fillna(hist_mean)\n",
    "    df['price_diff_hist'] = df['price_usd'] - df['hist_price_usd']\n",
    "    df['price_rat_hist']  = df['price_usd'] / (df['hist_price_usd'] + 1e-6)\n",
    "\n",
    "    # Competitor percent-diffs log transformation\n",
    "    diff_cols = [c for c in df.columns if 'rate_percent_diff' in c]\n",
    "    for c in diff_cols:\n",
    "        df[c] = df[c].fillna(0)\n",
    "        df[f'log_{c}'] = np.log1p(df[c].abs()) * np.sign(df[c])\n",
    "\n",
    "    # Flag missing historical stars\n",
    "    df['hist_star_na'] = df['visitor_hist_starrating'].isna().astype(int)\n",
    "    df['visitor_hist_starrating'] = df['visitor_hist_starrating'].fillna(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ac94a5-3ccb-4e37-b03f-6ecc7f2406af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply new featrues\n",
    "train_feat = create_base_features(train.copy())\n",
    "test_feat  = create_base_features(test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7450a3-d266-4bd7-8050-8105ed6ee139",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ create new features\n",
    "\n",
    "# Compute popularity and booking rate per destination\n",
    "dest_stats = (\n",
    "    train_feat\n",
    "    .groupby('srch_destination_id')\n",
    "    .agg(dest_searches=('srch_id', 'count'),\n",
    "         dest_bookings=('booking_bool', 'sum'))\n",
    "    .reset_index()\n",
    ")\n",
    "dest_stats['dest_booking_rate'] = dest_stats['dest_bookings'] / dest_stats['dest_searches']\n",
    "\n",
    "# merge new features\n",
    "for df in (train_feat, test_feat):\n",
    "    df.merge(dest_stats[['srch_destination_id','dest_searches','dest_booking_rate']],\n",
    "             on='srch_destination_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c655c5e7-e591-4975-8763-50dc9ebad62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank data\n",
    "for df in (train_feat, test_feat):\n",
    "    df['price_rank'] = df.groupby('srch_id')['price_usd'].rank(method='dense', ascending=True)\n",
    "    df['star_rank']  = df.groupby('srch_id')['prop_starrating'].rank(method='dense', ascending=False)\n",
    "    df['dist_rank']  = df.groupby('srch_id')['orig_destination_distance']\\\n",
    "                         .rank(method='dense', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50068faf-f786-4887-86e5-85d3fc98dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### create train and test data\n",
    "\n",
    "# Create relevance label for training\n",
    "y = train_feat['booking_bool'] * 5 + train_feat['click_bool']\n",
    "\n",
    "# List of features to drop (train-only or unneeded)\n",
    "drop_cols = ['date_time','click_bool','booking_bool','gross_bookings_usd','position']\n",
    "features = [c for c in train_feat.columns if c not in drop_cols + ['srch_id','prop_id']]\n",
    "\n",
    "#create X train and X_test\n",
    "X = train_feat[features]\n",
    "X_test = test_feat[features]\n",
    "\n",
    "# Extract unique query IDs so we can split at the search-session level\n",
    "group_ids = train_feat['srch_id'].unique()\n",
    "\n",
    "# train test split 80/20\n",
    "train_ids, valid_ids = train_test_split(\n",
    "    group_ids,\n",
    "    test_size=0.2,\n",
    "    random_state=22\n",
    ")\n",
    "\n",
    "# Build boolean masks that mark every row in train_feat as belonging\n",
    "#    either to the train split or the validation split, based on its srch_id\n",
    "mask_tr = train_feat['srch_id'].isin(train_ids)\n",
    "mask_va = train_feat['srch_id'].isin(valid_ids)\n",
    "\n",
    "# Subset your feature matrix X and label vector y according to those masks\n",
    "#    so X_tr/y_tr and X_va/y_va correspond to disjoint sets of queries\n",
    "X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "X_va, y_va = X[mask_va], y[mask_va]\n",
    "\n",
    "# Compute the “group sizes” for LightGBM’s ranker: for each query (srch_id),\n",
    "#    count how many candidate rows belong to it.  This array of counts\n",
    "#    tells the ranker where one query ends and the next begins.\n",
    "groups_tr = train_feat[mask_tr].groupby('srch_id').size().values\n",
    "groups_va = train_feat[mask_va].groupby('srch_id').size().values\n",
    "\n",
    "#get train and valdiation data to train the model\n",
    "train_data = lgb.Dataset(X_tr, label=y_tr, group=groups_tr)\n",
    "valid_data = lgb.Dataset(X_va, label=y_va, group=groups_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28bac05-c5ab-4228-9560-57df25d77448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's ndcg@5: 0.377543\n",
      "[200]\tvalid_0's ndcg@5: 0.382643\n",
      "[300]\tvalid_0's ndcg@5: 0.385069\n",
      "[400]\tvalid_0's ndcg@5: 0.38626\n",
      "[500]\tvalid_0's ndcg@5: 0.386712\n",
      "Early stopping, best iteration is:\n",
      "[500]\tvalid_0's ndcg@5: 0.386712\n"
     ]
    }
   ],
   "source": [
    "#run lgb model and set paramters\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "#dont make the model run endlesly \n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[valid_data],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)],\n",
    "    num_boost_round=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909b0772-bde3-4062-9292-d79a827ab7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict relevance scores for each test row\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Insert those scores into the sample submission DataFrame\n",
    "sample['score'] = preds\n",
    "\n",
    "#  Sort by search session (ascending) and score (descending)\n",
    "#    so that for each srch_id, the most relevant prop_id comes first\n",
    "submission = sample.sort_values(\n",
    "    ['srch_id', 'score'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# keep only the required columns and write to CSV\n",
    "#    Kaggle expects: srch_id, prop_id (in ranked order)\n",
    "submission[['srch_id', 'prop_id']].to_csv(\n",
    "    'submission.csv',\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
